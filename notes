spark souce code:github.com/apache/spark

RDD: A Resilient Distributed Dataset(RDD), basic abstraction in Spark. Represents an immutable,
partitioned collection of elements that can be opertaed on in parallel. This class contains the
basic operations available on all RDDs, such as 'map', 'filter', and 'persist'

1.RDD是一个抽象类：
  不能直接使用，子类实现

2.带泛型，可以支持多种类型：String, person, user

3.

RDD: A Resilient Distributed Dataset(RDD) 弹性 分布式 数据集
弹性: 容错，比如说一个节点挂了，他可以自动修复
分布式：跨节点分布式运行
数据集：一个集合
    immutable：不可变
    partitioned collection of elements：分区
        Array(1,2,3,4,5,6,7,8,9,10) 3个分区 (1,2,3),(4,5,6),(7,8,9,10)
    that can be opertaed on in parallel. 并行计算的问题


单机存储/计算  ==> 分布式存储/计算
1)数据的存储：切割   HDFS的Block
2)数据的计算：切割（分布式并行计算）  MapReduce/Spark
3)存储+计算:  HDFS/S3 + MapReduce/Spark



RDD的特性：
Internally, each RDD is characterized by five main properties:
  - A list of partitions
    一系列的分区/分片：HDFS一个文件由多个BLOCK构成

  - A function for computing each split/partition
    对RDD执行函数，其实就是对这个RDD里所有的分区执行相同的函数的操作
    y = f(x)
    rdd.map(_+1)

  - A list of dependencies on other RDDs
    rdd1 ==> rdd2 ==> rdd3 ==> rdd4
    rdd存在于依赖关系

    rddA = 5个partition
    ==> map
    rddB = 5个partition
    如果计算rddA里面第三个分区丢了，那根据依赖关系，重新计算rddA的第三分区（不是全部分区）



  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
    分区策略

  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
    an HDFS file)
    数据在哪，优先把作业调度到数据所在的节点进行计算：移动数据不如移动计算


五大特性源码体现：
def compute(split: Partition， context: TaskContext): Iterator[T] 特性二
protected def getPartitions: Array[Partition] 特性一
protected def getDependencies: Seq[Dependency[_]] = deps 特性三
protected def getPrefferedLocations(split: Partition): Seq[String] = Nil 特性�五
val partitioner: Option[Partitioner] = None 特性四


第一要务：创建SparkContext
     连接到spark集群：local，standalone,yarn,mesos
     通过SparkContext来创建RDD、广播变量到集群

  在创建SparkContext之前还需要创建一个SparkConf对象

  RDD创建方式：
    Parallelized Collections
    External Datasets

  If using a path on the local filesystem, the file must also be accessible at othersame path on worker nodes
    1.我们上课是在单节点上的：一个节点，hello.txt只要在这台机器上有就行了
    2.standalone:Spark集群：3个节点，local path都是从节点的本地读取数据
    所以用网络的文件最好>>> lines = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")


开发Pyspark应用程序
  1）IDE：IDEA pycharm
  2) 设置基本参数：
      环境增加SPARK_HOME，Python interceptor, PYTHONPATH, 2个zip包
  3）开发
  4）使用LOCAL进行本地测试

提交pyspark应用程序($spark_HOME)
    ./spark-submit --master local[2] --name spark0301 /home/hadoop/script/spark0301.py
    具体提交的相信说明参见：http://spark.apache.org/docs/latest/submitting-applications.html


RDD常用操作(operations)：
  - transformations: create a new dataset from an existing one
      RDDA -- transformation --> RDDB

      y = f(x)
      rddb=rdda.map(....)



      lazy:
        只存在关系，不生成，直到下次调用
      rdda.map().filer()......collect

      map/filter/group by/distinct.....


  - actions:
        return a value to the driver program after running a computation on the dataset
        count/reduce/collect....


    1/transformation are lazy, nothing actually happens until an action is called
    2/action trigger the cpmputation;
    3/action returns values to driver or writes data to external storage


map:
    map(func)
    将func函数作用数据集每一个元素上，生成一个新的分布式的数据集返回

filter:
    filter(func)
    选出所有func返回值为true的元素，返回所有返回值为true的值

flatMap:
    flatMap(func)
    输入的item能够被map到0或者多个items输出，返回值是一个Sequence
    压扁输出（例如["hello world","hello python","hello spark"]
    直接用Map输出，其实是['hello', 'world']
                      ['hello', 'sql']
                      ['hello', 'spark']
    拍扁了用flatmap就是['hello', 'world','hello', 'sql','hello', 'spark']
    但是flatmap只在spark里有

groupByKey():
    mapRdd.groupByKey(),数据结构一定是要有Key的！
    把key相同的放在一起

reduceByKey:
    把key相同的放在一起，并且进行相应计算
    maprdd.reduceByKey(lamda a,b:a+b)

Python简易word count:
    data=["xXX","XXX","XXX"]
    priint(data.map(lambda x:(x,1)).reduceByKey(lamda a,b:a+b).collect())

需求：请按WC结果中出现的次数升序排列
    sorted(data,key=lambda x:x[1],reverse=True)

    spark里有sortByKey(),但是如何用value排序？
    map(lambda x:(x[1],x[0])).sortByKey().map(lambda x:(x[0],x[1])).collect()
    想法就是：各种用map进行输出转换

union:
    a = sc.parallelize([1,2,3])
    b = sc.parallelize([3,4,5])
    a.union(b).collect()
    ==> [1,2,3,3,4,5]

distinct:
    a = sc.parallelize([1,2,3])
    b = sc.parallelize([3,4,5])
    a.union(b).distinct().collect()
    ==> [1,2,3,4,5]

join:
    inner join: a.join(b).collect()
    left join: a.leftOuterJoin(b).collect()  PS:如果B没有，就会变NONE("c",(A的值，B的值))，NONE不能省略
    right join: a.rightOuterJoin(b).collect()


RDD常用actions：
    collect, count, take, reduce, saveAsTextFile, foreach


词频分析案例：WC
    1.input : 1/n文件，文件夹， 后缀名
      hello spark
      hello hadoop
      hello welcome
    2.开发步骤：
      文本内容的每一行转成一个个的单词: flatMap
      单词 ==> (单词，1)： map
      把所有相同单词的技术相加得到最终的结果: reduceByKey

      if len(sys.argv) !=2
        print("Usage: wordcount <input>",file = sys.stderr)
        sys.exit(-1)

      conf = SparkConf()
      sc = SparkContext( conf = conf )


      def printResult():
        counts = sc.textFile(sys.argv[1])\
          .flatMap(lambda line:line.split('\t'))\
          .map(lambda x:(x,1)).reduceByKey(lambda a,b:a + b)

        output = counts.collect()
        for (word, count) in output:
          print("%s: %i " % (word, count))


      def saveFile():
        counts = sc.textFile(sys.argv[1])\
          .flatMap(lambda line:line.split('\t'))\
          .map(lambda x:(x,1)).reduceByKey(lambda a,b:a + b)\
          .saveAsTextFile(sys.argv[2])

      sc.stop()

TopN:
      1.input : 1/n文件，文件夹， 后缀名
      2.求某个维度的TopN
      3.开发步骤：
          文本内容的每一行根据需求提出你所需要的字段：map
          单词 ==> （单词,1):map
          把所有相同单词的计数相加得到最终的结果:reduceByKey
          取最多出现次数的降序：sortByKey


      if len(sys.argv) !=2
        print("Usage: TopN <input>",file = sys.stderr)
        sys.exit(-1)

      conf = SparkConf()
      sc = SparkContext( conf = conf )

      coounts = sc.textFile(sys.argv[1])\
          .map(lambda x:x.split('\t'))\
          .map(lambda x:(x[5],1))\
          .reduceByKey(lambda a,b: a+b)\
          .map(lambda x:(x[1],x[0]))\
          .sortByKey(False)\
          .map(lambda x:(x[1],x[0])).take(5)
      sc.stop()


平均数：统计平均年龄
      开发步骤分析：
        1.取出年龄 map
        2.计算年龄总和 reduce
        3.计算记录总数 count
        4.计算平均数


        if len(sys.argv) !=2
          print("Usage: TopN <input>",file = sys.stderr)
          sys.exit(-1)

        conf = SparkConf()
        sc = SparkContext( conf = conf )
        ageData = sc.textFile(sys.argv[1]).map(lambda x:x.split(" ")[1])
        totalAge = ageData.map(lamda age:int(age)).reduce(lambda a,b:a+b)
        counts = ageData.count()
        avgAge = totalAge/counts


Spark特性
    Application : 基于Spark的应用程序 = 1 driver + executors
        User program build on Spark.
        Consists of a dtiver program and executors on the cluster.
        pyspark/spark-shell 就是应用程序

    Driver program:
        The process running the main() function of the application creating the SparkContext

    Cluster manager: 申请资源
         An external service for acquireing resources onthe cluster (standalone, yarn)
         spark-submit --master local[2]/spark://hadoop000:7077/yarn

    Deploy mode: 运行模式, 本地就是client，集群就是cluster
        Distinguishes where the driver process runs.
        In "cluster" mode, the framework launches the driver inside of the cluster.
        In "client" mode, the submitted launches the driver outside of the cluster.

    Worker node:
        Any node that can run application code in the Cluster
        standalone: slave节点 salves配置文件
        yarn: nodemanager

    executor:
        A process launched for an application on a woker node
        run tasks
        keeps data in memory or disk storage across them
        each application has its own executors

    Task:
        A unit of work that will be sent to one executor

    Job:
        A parallel computation consisting of multiple tasks that
        gets spawned in response to a Spark action(e.g. save, collect);
        you'll see this term used in the driver's logs.
        一个action对应一个job

    Stage:
        Each job gets divided into smaller sets of tasks called stages
        that depend on each other
        (similar to the map and reduce stages in MapReduce);
        you'll see this term used in the driver's logs.
        一个stage的边界往往是从某个取数据开始，到shuffle结束。

    spark和hadoop的重要区别：
    Hadoop
        1.一个MR程序 = 一个JOB
        2.一个JOB = 一个N个TASK（Map/Reduce)
        3.一个Task对应于一个进程
        4.Task运行时开启进程，Task执行完毕后销毁进程，对于多个Task来说
          开销是比较大的（即便通过JMV共享）
    Spark
        1.Application = Driver(main方法中创建sparkContext) + Executors
        2.一个Application = 0到多个JOB
        3.一个JOB = 1个ACTION
        4.一个JOB = 1到N个Stage
        5.一个Stage = 1到N个Task
        6.一个Task对应一个线程，多个Task可以以并行的方式运行在一个Executor进程中

Spark Cache()
    rdd.cache():Storagelevel

    cache和transformation : Lazy机制，没有遇到action是不会提交作业到spark上运行的

    如果一个RDD在后续的计算中可能会被使用到，那么建议cache
    两次count行数，步骤：
    1.从文件系统读数据
    2.第一次count
    3.第二次count
    不缓存的场景：有多少次action就会读多少次disk上的数据
    有缓存：直接从cache读，减少磁盘IO
RDD Persisting/caching：
    cache底层调用了persist方法，传入的参数是Storagelevel.MEMORY_ONLY
    cache = persist
        使用方法：lines.persist(Storagelevel.MEMORY_ONLY_2)
    手动删除缓存方式.unpersist, spark自动采用LRU（Least recently used)处理缓存
    unpersist:立刻执行，不是LAZY的

    缓存策略机制：MEMORY_ONLY > MEMORY_ONLY_SER > 硬盘， 如果想快速错误recovery, 使用副本

Spark Lineage:RDD之间的依赖关系
    HDFS -> filter -> map -> 其他算子 ->

窄(Narrow)依赖：一个父RDD的partition之多被子RDD的某个partition使用一次。父RDD是指靠前的RDD，可以做pipeline-able
宽(Wide)依赖：一个父RDD的partition会被子RDD的partition使用多次，有shuffle
