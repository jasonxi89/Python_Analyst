spark souce code:github.com/apache/spark

RDD: A Resilient Distributed Dataset(RDD), basic abstraction in Spark. Represents an immutable,
partitioned collection of elements that can be opertaed on in parallel. This class contains the
basic operations available on all RDDs, such as 'map', 'filter', and 'persist'

1.RDD是一个抽象类：
  不能直接使用，子类实现

2.带泛型，可以支持多种类型：String, person, user

3.

RDD: A Resilient Distributed Dataset(RDD) 弹性 分布式 数据集
弹性: 容错，比如说一个节点挂了，他可以自动修复
分布式：跨节点分布式运行
数据集：一个集合
    immutable：不可变
    partitioned collection of elements：分区
        Array(1,2,3,4,5,6,7,8,9,10) 3个分区 (1,2,3),(4,5,6),(7,8,9,10)
    that can be opertaed on in parallel. 并行计算的问题


单机存储/计算  ==> 分布式存储/计算
1)数据的存储：切割   HDFS的Block
2)数据的计算：切割（分布式并行计算）  MapReduce/Spark
3)存储+计算:  HDFS/S3 + MapReduce/Spark



RDD的特性：
Internally, each RDD is characterized by five main properties:
  - A list of partitions
    一系列的分区/分片：HDFS一个文件由多个BLOCK构成

  - A function for computing each split/partition
    对RDD执行函数，其实就是对这个RDD里所有的分区执行相同的函数的操作
    y = f(x)
    rdd.map(_+1)

  - A list of dependencies on other RDDs
    rdd1 ==> rdd2 ==> rdd3 ==> rdd4
    rdd存在于依赖关系

    rddA = 5个partition
    ==> map
    rddB = 5个partition
    如果计算rddA里面第三个分区丢了，那根据依赖关系，重新计算rddA的第三分区（不是全部分区）



  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
    分区策略

  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
    an HDFS file)
    数据在哪，优先把作业调度到数据所在的节点进行计算：移动数据不如移动计算


五大特性源码体现：
def compute(split: Partition， context: TaskContext): Iterator[T] 特性二
protected def getPartitions: Array[Partition] 特性一
protected def getDependencies: Seq[Dependency[_]] = deps 特性三
protected def getPrefferedLocations(split: Partition): Seq[String] = Nil 特性�五
val partitioner: Option[Partitioner] = None 特性四


第一要务：创建SparkContext
     连接到spark集群：local，standalone,yarn,mesos
     通过SparkContext来创建RDD、广播变量到集群

  在创建SparkContext之前还需要创建一个SparkConf对象

  RDD创建方式：
    Parallelized Collections
    External Datasets

  If using a path on the local filesystem, the file must also be accessible at othersame path on worker nodes
    1.我们上课是在单节点上的：一个节点，hello.txt只要在这台机器上有就行了
    2.standalone:Spark集群：3个节点，local path都是从节点的本地读取数据
    所以用网络的文件最好>>> lines = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")


开发Pyspark应用程序
  1）IDE：IDEA pycharm
  2) 设置基本参数：
      环境增加SPARK_HOME，Python interceptor, PYTHONPATH, 2个zip包
  3）开发
  4）使用LOCAL进行本地测试

提交pyspark应用程序($spark_HOME)
    ./spark-submit --master local[2] --name spark0301 /home/hadoop/script/spark0301.py
    具体提交的相信说明参见：http://spark.apache.org/docs/latest/submitting-applications.html


RDD常用操作(operations)：
  - transformations: create a new dataset from an existing one
      RDDA -- transformation --> RDDB

      y = f(x)
      rddb=rdda.map(....)



      lazy:
        只存在关系，不生成，直到下次调用
      rdda.map().filer()......collect

      map/filter/group by/distinct.....


  - actions:
        return a value to the driver program after running a computation on the dataset
        count/reduce/collect....


    1/transformation are lazy, nothing actually happens until an action is called
    2/action trigger the cpmputation;
    3/action returns values to driver or writes data to external storage


map:
    map(func)
    将func函数作用数据集每一个元素上，生成一个新的分布式的数据集返回

filter:
    filter(func)
    选出所有func返回值为true的元素，返回所有返回值为true的值

flatMap:
    flatMap(func)
    输入的item能够被map到0或者多个items输出，返回值是一个Sequence
    压扁输出（例如["hello world","hello python","hello spark"]
    直接用Map输出，其实是['hello', 'world']
                      ['hello', 'sql']
                      ['hello', 'spark']
    拍扁了用flatmap就是['hello', 'world','hello', 'sql','hello', 'spark']
    但是flatmap只在spark里有

groupByKey():
    mapRdd.groupByKey(),数据结构一定是要有Key的！
    把key相同的放在一起

reduceByKey:
    把key相同的放在一起，并且进行相应计算
    maprdd.reduceByKey(lamda a,b:a+b)

Python简易word count:
    data=["xXX","XXX","XXX"]
    priint(data.map(lambda x:(x,1)).reduceByKey(lamda a,b:a+b).collect())

需求：请按WC结果中出现的次数升序排列
    sorted(data,key=lambda x:x[1],reverse=True)

    spark里有sortByKey(),但是如何用value排序？
    map(lambda x:(x[1],x[0])).sortByKey().map(lambda x:(x[0],x[1])).collect()
    想法就是：各种用map进行输出转换

union:
    a = sc.parallelize([1,2,3])
    b = sc.parallelize([3,4,5])
    a.union(b).collect()
    ==> [1,2,3,3,4,5]

distinct:
    a = sc.parallelize([1,2,3])
    b = sc.parallelize([3,4,5])
    a.union(b).distinct().collect()
    ==> [1,2,3,4,5]

join:
    inner join: a.join(b).collect()
    left join: a.leftOuterJoin(b).collect()  PS:如果B没有，就会变NONE("c",(A的值，B的值))，NONE不能省略
    right join: a.rightOuterJoin(b).collect()


RDD常用actions：
    collect, count, take, reduce, saveAsTextFile, foreach


词频分析案例：WC
    1.input : 1/n文件，文件夹， 后缀名
      hello spark
      hello hadoop
      hello welcome
    2.开发步骤：
      文本内容的每一行转成一个个的单词: flatMap
      单词 ==> (单词，1)： map
      把所有相同单词的技术相加得到最终的结果: reduceByKey

      if len(sys.argv) !=2
        print("Usage: wordcount <input>",file = sys.stderr)
        sys.exit(-1)

      conf = SparkConf()
      sc = SparkContext( conf = conf )


      def printResult():
        counts = sc.textFile(sys.argv[1])\
          .flatMap(lambda line:line.split('\t'))\
          .map(lambda x:(x,1)).reduceByKey(lambda a,b:a + b)

        output = counts.collect()
        for (word, count) in output:
          print("%s: %i " % (word, count))


      def saveFile():
        counts = sc.textFile(sys.argv[1])\
          .flatMap(lambda line:line.split('\t'))\
          .map(lambda x:(x,1)).reduceByKey(lambda a,b:a + b)\
          .saveAsTextFile(sys.argv[2])

      sc.stop()

TopN:
      1.input : 1/n文件，文件夹， 后缀名
      2.求某个维度的TopN
      3.开发步骤：
          文本内容的每一行根据需求提出你所需要的字段：map
          单词 ==> （单词,1):map
          把所有相同单词的计数相加得到最终的结果:reduceByKey
          取最多出现次数的降序：sortByKey


      if len(sys.argv) !=2
        print("Usage: TopN <input>",file = sys.stderr)
        sys.exit(-1)

      conf = SparkConf()
      sc = SparkContext( conf = conf )

      coounts = sc.textFile(sys.argv[1])\
          .map(lambda x:x.split('\t'))\
          .map(lambda x:(x[5],1))\
          .reduceByKey(lambda a,b: a+b)\
          .map(lambda x:(x[1],x[0]))\
          .sortByKey(False)\
          .map(lambda x:(x[1],x[0])).take(5)
      sc.stop()


平均数：统计平均年龄
      开发步骤分析：
        1.取出年龄 map
        2.计算年龄总和 reduce
        3.计算记录总数 count
        4.计算平均数


        if len(sys.argv) !=2
          print("Usage: TopN <input>",file = sys.stderr)
          sys.exit(-1)

        conf = SparkConf()
        sc = SparkContext( conf = conf )
        ageData = sc.textFile(sys.argv[1]).map(lambda x:x.split(" ")[1])
        totalAge = ageData.map(lamda age:int(age)).reduce(lambda a,b:a+b)
        counts = ageData.count()
        avgAge = totalAge/counts


Spark特性
    Application : 基于Spark的应用程序 = 1 driver + executors
        User program build on Spark.
        Consists of a dtiver program and executors on the cluster.
        pyspark/spark-shell 就是应用程序

    Driver program:
        The process running the main() function of the application creating the SparkContext

    Cluster manager: 申请资源
         An external service for acquireing resources onthe cluster (standalone, yarn)
         spark-submit --master local[2]/spark://hadoop000:7077/yarn

    Deploy mode: 运行模式, 本地就是client，集群就是cluster
        Distinguishes where the driver process runs.
        In "cluster" mode, the framework launches the driver inside of the cluster.
        In "client" mode, the submitted launches the driver outside of the cluster.

    Worker node:
        Any node that can run application code in the Cluster
        standalone: slave节点 salves配置文件
        yarn: nodemanager

    executor:
        A process launched for an application on a woker node
        run tasks
        keeps data in memory or disk storage across them
        each application has its own executors

    Task:
        A unit of work that will be sent to one executor

    Job:
        A parallel computation consisting of multiple tasks that
        gets spawned in response to a Spark action(e.g. save, collect);
        you'll see this term used in the driver's logs.
        一个action对应一个job

    Stage:
        Each job gets divided into smaller sets of tasks called stages
        that depend on each other
        (similar to the map and reduce stages in MapReduce);
        you'll see this term used in the driver's logs.
        一个stage的边界往往是从某个取数据开始，到shuffle结束。

    spark和hadoop的重要区别：
    Hadoop
        1.一个MR程序 = 一个JOB
        2.一个JOB = 一个N个TASK（Map/Reduce)
        3.一个Task对应于一个进程
        4.Task运行时开启进程，Task执行完毕后销毁进程，对于多个Task来说
          开销是比较大的（即便通过JMV共享）
    Spark
        1.Application = Driver(main方法中创建sparkContext) + Executors
        2.一个Application = 0到多个JOB
        3.一个JOB = 1个ACTION
        4.一个JOB = 1到N个Stage
        5.一个Stage = 1到N个Task
        6.一个Task对应一个线程，多个Task可以以并行的方式运行在一个Executor进程中

Spark Cache()
    rdd.cache():Storagelevel

    cache和transformation : Lazy机制，没有遇到action是不会提交作业到spark上运行的

    如果一个RDD在后续的计算中可能会被使用到，那么建议cache
    两次count行数，步骤：
    1.从文件系统读数据
    2.第一次count
    3.第二次count
    不缓存的场景：有多少次action就会读多少次disk上的数据
    有缓存：直接从cache读，减少磁盘IO
RDD Persisting/caching：
    cache底层调用了persist方法，传入的参数是Storagelevel.MEMORY_ONLY
    cache = persist
        使用方法：lines.persist(Storagelevel.MEMORY_ONLY_2)
    手动删除缓存方式.unpersist, spark自动采用LRU（Least recently used)处理缓存
    unpersist:立刻执行，不是LAZY的

    缓存策略机制：MEMORY_ONLY > MEMORY_ONLY_SER > 硬盘， 如果想快速错误recovery, 使用副本

Spark Lineage:RDD之间的依赖关系
    HDFS -> filter -> map -> 其他算子 ->

窄(Narrow)依赖：一个父RDD的partition之多被子RDD的某个partition使用一次。父RDD是指靠前的RDD，可以做pipeline-able
宽(Wide)依赖：一个父RDD的partition会被子RDD的partition使用多次，有shuffle
宽依赖的容错率比较低，容错很复杂

宽依赖shuffle触发：reduceByKey,各种ByKey，join，并且shuffle非常昂贵，涉及到disk I/O, data serialization, network I/O


PySpark实战之spark优化
    historyserver: 记录日志
        ￥SPARK_HOME/conf/spark-default.conf
            spark.eventLog.enable     true
            spark.eventLog.dir        hdfs://hadoop000:8020/dictionary
    序列化serialized：
        1.序列化的作用：1网络传传输，进程之间相互通信
            Java序列号：类支持好，但是慢
            Kryo:高效，但是支持不好
    内存管理：
        1.对象使用了多少
        2.访问对象使用多少
        3.垃圾回收
        分为两大类：Execution执行（运算），Storage存储(cache)
        share a unified region，但是运算必要的时候，可以剔除存储，但是存储可以设个最小值
    广播变量(Broadcast Variables):
        sc.Broadcast Variables
    数据本地化(data locality):
        移动数据还是移动计算？移动计算而不是数据，因为计算比数据小很多。
        If data and the code that operates on it are together then computation
        tends to be fast. But if code and data are separated, one must move to the other.
        Typically it is faster to ship serialized code from place to place than
        a chunk of data because code size is much smaller than data. Spark builds
        its scheduling around this general principle of data locality.

        PROCESS_LOCAL > Node_LOCAL > NO_PREF > RACK_LOCAL > ANY


Spark SQL:
     SQL: MySQL, Oracle, DB2, SQLServer
     很多小虎斑熟悉SQL语言
     数据量越来越大 ==> 大数据（Hive，Spark Core)
     直接使用SQL语句来对大数据进行分析：这是大家所追逐的梦想

     person.txt ==> 存放在HDFS
     1,zhangsan,30
     2,lisi,31
     3,wangwu,32

     Hive表：person
        id:int name:string age:int
     导入数据：
        load ....
      统计分析
        select ... from person
SQL on hadoop:
    Hive
    Spark
    Impala: Cloudera
    Presto
    Drill

Hive: on MapReduce
    SQL ==> MapReduce ==> Hadoop Cluster

Shark: on Spark
    基于Hive 源码进行改造

Spark SQL: on Spark

Hive on Spark: 底层Hive

共同点： metastore(表的源数据信息) MySQL,

Spark SQL概述：
     Apache Spark's module for working with structured data.
     1.Integrated：集成
        可以用SQL语句 或者 DATAFRAME API
     2.Uniform Data Access： 统一的链接
        访问各种数据源/join different data sources.
     3.Hive Integration:
        Run SQL or HiveQL queries on existing warehouses
     4.Standard Connectivity:
        JDBC/ODBC

     Spark SQL不仅仅是SQL这么简单的事情，它还能做更多的事情
        Hive:SQL只是查询，底层架构完全不一样
        Spark SQL:不仅仅是SQL，还有DataFrame API，Dataset API

     一个用于处理结构化数据的spark组件，强调的是“结构化数据”，而非”SQL“

     Datasets and DataFrames:
        Dataset: a distributed collection of data. Python not supported.

        DataFrame: is a Dataset organized into named columns
        以列的方式构成分布式的数据集

        面试题：RDD与DataFrame的区别
        https://www.zhihu.com/question/48684460


Spark Streaming:
     is an exgension of the core Spark API
     enables scalable, high-throughtput, fault-tolerant
     steam processing of live data streams

     基于RDD，spark core的扩展，一个流处理

     Streaming: Input/ Output
        Input: Kafka, Flume, Kinesis, or TCP sockets,
        Output: filesystems(HDFS), databases, and live dashboards


     常用实时流处理框架对比：
        Storm: 真正的流处理 Tuple
        Spark Streaming: 并不是真正的实时流处理，而是一个mini batch操作
        Flink: 实时流处理，和spark底层相反
        kafak Stream:


     Spark Streaming:
        receives live input data streams and
        divides the data into batches, which are
        then processed by the Spark engine to
        generate the final stream of results in batches.

      Spark Core的核心抽象：RDD，五大特性，对应源码中的5个方法是什么
      Spark Streaming的核心抽象叫：DStream
          represents a continuous stream of data.
          DStreams can be created either from input data streams from sources such as Kafka, Flume, and Kinesis,
          or by applying high-level operations on other DStreams.
          Internally, a DStream is represented as a sequence of RDDs.

      StreamingContext


项目实战：
大数据项目开发流程：
1.调研
    业务为导向
2.需求分析
    项目的需求
       显式需求
       隐式需求
    甘特图：项目周期管理
3.方案设计
    概要设计
    详细设计
        基本要求
        系统要求：扩展性、容错性、高可用（HDFS YARN）、定制化
4.功能开发
    开发
    单元测试 junit
5.测试
    测试环境 QA
    功能、性能、压力
    用户测试
6.部署上线
    试运行 DIFF ”双活“
    正式上线
7.运维
    7 * 24
8.后期迭代开发

大数据企业级应用：
  1.数据分析
      数据平台：
          商业：友盟(umeng)
          自研：
  2.搜索/引擎
      Lucene/Solr/ELK
  3.机器学期
  4.精准营销
  5.人工智能


企业级大数据分析平台
  1.商业
    费用高，数据安全，定制性低
  2.自研
    Apache, CDH, HDP
    二次开发，高度定制，需要技术能力支持  


数据量预估以及集群规划
Q：一条日志多少、多少个字段、一天多少数据
  300-500字节 * 1000W * 5 * 5 = 100G
  HDFS 3副本 * 100G = 300G * （2-3年）


服务器一台：磁盘多少 ==> Node 数量
    集群规模：数据量 + 存储周期


集群机器规模：
    DN：数据量大小/每个Node的磁盘大小
    NM: 2
    RM: 2
    NM: DN
    ZK：3/5/7/9
    GATEWAY:

资源设置：cpu/memory/disk/network

作业规划：
    MapReduce/Hive/Spark
    Server：
    调度：AZ、OOZIE


项目实战：
  需求：使用PySpark分析空气质量
  数据来源：stateair.net/web/historical/1/1.html
  根据北京的数据进行统计分析
