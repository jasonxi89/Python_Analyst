spark souce code:github.com/apache/spark

RDD: A Resilient Distributed Dataset(RDD), basic abstraction in Spark. Represents an immutable,
partitioned collection of elements that can be opertaed on in parallel. This class contains the
basic operations available on all RDDs, such as 'map', 'filter', and 'persist'

1.RDD是一个抽象类：
  不能直接使用，子类实现

2.带泛型，可以支持多种类型：String, person, user

3.

RDD: A Resilient Distributed Dataset(RDD) 弹性 分布式 数据集
弹性: 容错，比如说一个节点挂了，他可以自动修复
分布式：跨节点分布式运行
数据集：一个集合
    immutable：不可变
    partitioned collection of elements：分区
        Array(1,2,3,4,5,6,7,8,9,10) 3个分区 (1,2,3),(4,5,6),(7,8,9,10)
    that can be opertaed on in parallel. 并行计算的问题


单机存储/计算  ==> 分布式存储/计算
1)数据的存储：切割   HDFS的Block
2)数据的计算：切割（分布式并行计算）  MapReduce/Spark
3)存储+计算:  HDFS/S3 + MapReduce/Spark



RDD的特性：
Internally, each RDD is characterized by five main properties:
  - A list of partitions
    一系列的分区/分片：HDFS一个文件由多个BLOCK构成

  - A function for computing each split/partition
    对RDD执行函数，其实就是对这个RDD里所有的分区执行相同的函数的操作
    y = f(x)
    rdd.map(_+1)

  - A list of dependencies on other RDDs
    rdd1 ==> rdd2 ==> rdd3 ==> rdd4
    rdd存在于依赖关系

    rddA = 5个partition
    ==> map
    rddB = 5个partition
    如果计算rddA里面第三个分区丢了，那根据依赖关系，重新计算rddA的第三分区（不是全部分区）



  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
    分区策略

  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
    an HDFS file)
    数据在哪，优先把作业调度到数据所在的节点进行计算：移动数据不如移动计算


五大特性源码体现：
def compute(split: Partition， context: TaskContext): Iterator[T] 特性二
protected def getPartitions: Array[Partition] 特性一
protected def getDependencies: Seq[Dependency[_]] = deps 特性三
protected def getPrefferedLocations(split: Partition): Seq[String] = Nil 特性�五
val partitioner: Option[Partitioner] = None 特性四


第一要务：创建SparkContext
     连接到spark集群：local，standalone,yarn,mesos
     通过SparkContext来创建RDD、广播变量到集群

  在创建SparkContext之前还需要创建一个SparkConf对象

  RDD创建方式：
    Parallelized Collections
    External Datasets

  If using a path on the local filesystem, the file must also be accessible at othersame path on worker nodes
    1.我们上课是在单节点上的：一个节点，hello.txt只要在这台机器上有就行了
    2.standalone:Spark集群：3个节点，local path都是从节点的本地读取数据
    所以用网络的文件最好>>> lines = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")


开发Pyspark应用程序
  1）IDE：IDEA pycharm
  2) 设置基本参数：
      环境增加SPARK_HOME，Python interceptor, PYTHONPATH, 2个zip包
  3）开发
  4）使用LOCAL进行本地测试

提交pyspark应用程序($spark_HOME)
    ./spark-submit --master local[2] --name spark0301 /home/hadoop/script/spark0301.py
    具体提交的相信说明参见：http://spark.apache.org/docs/latest/submitting-applications.html
