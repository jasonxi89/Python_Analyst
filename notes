spark souce code:github.com/apache/spark

RDD: A Resilient Distributed Dataset(RDD), basic abstraction in Spark. Represents an immutable,
partitioned collection of elements that can be opertaed on in parallel. This class contains the
basic operations available on all RDDs, such as 'map', 'filter', and 'persist'

1.RDD是一个抽象类：
  不能直接使用，子类实现

2.带泛型，可以支持多种类型：String, person, user

3.

RDD: A Resilient Distributed Dataset(RDD) 弹性 分布式 数据集
弹性: 容错，比如说一个节点挂了，他可以自动修复
分布式：跨节点分布式运行
数据集：一个集合
    immutable：不可变
    partitioned collection of elements：分区
        Array(1,2,3,4,5,6,7,8,9,10) 3个分区 (1,2,3),(4,5,6),(7,8,9,10)
    that can be opertaed on in parallel. 并行计算的问题


单机存储/计算  ==> 分布式存储/计算
1)数据的存储：切割   HDFS的Block
2)数据的计算：切割（分布式并行计算）  MapReduce/Spark
3)存储+计算:  HDFS/S3 + MapReduce/Spark



RDD的特性：
Internally, each RDD is characterized by five main properties:
  - A list of partitions
    一系列的分区/分片：HDFS一个文件由多个BLOCK构成

  - A function for computing each split/partition
    对RDD执行函数，其实就是对这个RDD里所有的分区执行相同的函数的操作
    y = f(x)
    rdd.map(_+1)

  - A list of dependencies on other RDDs
    rdd1 ==> rdd2 ==> rdd3 ==> rdd4
    rdd存在于依赖关系

    rddA = 5个partition
    ==> map
    rddB = 5个partition
    如果计算rddA里面第三个分区丢了，那根据依赖关系，重新计算rddA的第三分区（不是全部分区）



  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
    分区策略

  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
    an HDFS file)
    数据在哪，优先把作业调度到数据所在的节点进行计算：移动数据不如移动计算


五大特性源码体现：
def compute(split: Partition， context: TaskContext): Iterator[T] 特性二
protected def getPartitions: Array[Partition] 特性一
protected def getDependencies: Seq[Dependency[_]] = deps 特性三
protected def getPrefferedLocations(split: Partition): Seq[String] = Nil 特性�五
val partitioner: Option[Partitioner] = None 特性四


第一要务：创建SparkContext
     连接到spark集群：local，standalone,yarn,mesos
     通过SparkContext来创建RDD、广播变量到集群

  在创建SparkContext之前还需要创建一个SparkConf对象

  RDD创建方式：
    Parallelized Collections
    External Datasets

  If using a path on the local filesystem, the file must also be accessible at othersame path on worker nodes
    1.我们上课是在单节点上的：一个节点，hello.txt只要在这台机器上有就行了
    2.standalone:Spark集群：3个节点，local path都是从节点的本地读取数据
    所以用网络的文件最好>>> lines = sc.textFile("hdfs://localhost:9000/user/hadoop/word.txt")


开发Pyspark应用程序
  1）IDE：IDEA pycharm
  2) 设置基本参数：
      环境增加SPARK_HOME，Python interceptor, PYTHONPATH, 2个zip包
  3）开发
  4）使用LOCAL进行本地测试

提交pyspark应用程序($spark_HOME)
    ./spark-submit --master local[2] --name spark0301 /home/hadoop/script/spark0301.py
    具体提交的相信说明参见：http://spark.apache.org/docs/latest/submitting-applications.html


RDD常用操作(operations)：
  - transformations: create a new dataset from an existing one
      RDDA -- transformation --> RDDB

      y = f(x)
      rddb=rdda.map(....)



      lazy:
        只存在关系，不生成，直到下次调用
      rdda.map().filer()......collect

      map/filter/group by/distinct.....


  - actions:
        return a value to the driver program after running a computation on the dataset
        count/reduce/collect....


    1/transformation are lazy, nothing actually happens until an action is called
    2/action trigger the cpmputation;
    3/action returns values to driver or writes data to external storage


map:
    map(func)
    将func函数作用数据集每一个元素上，生成一个新的分布式的数据集返回

filter:
    filter(func)
    选出所有func返回值为true的元素，返回所有返回值为true的值

flatMap:
    flatMap(func)
    输入的item能够被map到0或者多个items输出，返回值是一个Sequence
    压扁输出（例如["hello world","hello python","hello spark"]
    直接用Map输出，其实是['hello', 'world']
                      ['hello', 'sql']
                      ['hello', 'spark']
    拍扁了用flatmap就是['hello', 'world','hello', 'sql','hello', 'spark']
    但是flatmap只在spark里有

groupByKey():
    mapRdd.groupByKey(),数据结构一定是要有Key的！
    把key相同的放在一起

reduceByKey:
    把key相同的放在一起，并且进行相应计算
    maprdd.reduceByKey(lamda a,b:a+b)

Python简易word count:
    data=["xXX","XXX","XXX"]
    priint(data.map(lambda x:(x,1)).reduceByKey(lamda a,b:a+b).collect())

需求：请按WC结果中出现的次数升序排列
    sorted(data,key=lambda x:x[1],reverse=True)

    spark里有sortByKey(),但是如何用value排序？
    map(lambda x:(x[1],x[0])).sortByKey().map(lambda x:(x[0],x[1])).collect()
    想法就是：各种用map进行输出转换

union:
    a = sc.parallelize([1,2,3])
    b = sc.parallelize([3,4,5])
    a.union(b).collect()
    ==> [1,2,3,3,4,5]

distinct:
    a = sc.parallelize([1,2,3])
    b = sc.parallelize([3,4,5])
    a.union(b).distinct().collect()
    ==> [1,2,3,4,5]

join:
    inner join: a.join(b).collect()
    left join: a.leftOuterJoin(b).collect()  PS:如果B没有，就会变NONE("c",(A的值，B的值))，NONE不能省略
    right join: a.rightOuterJoin(b).collect()


RDD常用actions：
    collect, count, take, reduce, saveAsTextFile, foreach


词频分析案例：WC
    1.input : 1/n文件，文件夹， 后缀名
      hello spark
      hello hadoop
      hello welcome
    2.开发步骤：
      文本内容的每一行转成一个个的单词: flatMap
      单词 ==> (单词，1)： map
      把所有相同单词的技术相加得到最终的结果: reduceByKey

      if len(sys.argv) !=2
        print("Usage: wordcount <input>",file = sys.stderr)
        sys.exit(-1)

      conf = SparkConf()
      sc = SparkContext( conf = conf )


      def printResult():
        counts = sc.textFile(sys.argv[1])\
          .flatMap(lambda line:line.split('\t'))\
          .map(lambda x:(x,1)).reduceByKey(lambda a,b:a + b)

        output = counts.collect()
        for (word, count) in output:
          print("%s: %i " % (word, count))


      def saveFile():
        counts = sc.textFile(sys.argv[1])\
          .flatMap(lambda line:line.split('\t'))\
          .map(lambda x:(x,1)).reduceByKey(lambda a,b:a + b)\
          .saveAsTextFile(sys.argv[2])

      sc.stop()

TopN:
      1.input : 1/n文件，文件夹， 后缀名
      2.求某个维度的TopN
      3.开发步骤：
          文本内容的每一行根据需求提出你所需要的字段：map
          单词 ==> （单词,1):map
          把所有相同单词的计数相加得到最终的结果:reduceByKey
          取最多出现次数的降序：sortByKey


      if len(sys.argv) !=2
        print("Usage: TopN <input>",file = sys.stderr)
        sys.exit(-1)

      conf = SparkConf()
      sc = SparkContext( conf = conf )

      coounts = sc.textFile(sys.argv[1])\
          .map(lambda x:x.split('\t'))\
          .map(lambda x:(x[5],1))\
          .reduceByKey(lambda a,b: a+b)\
          .map(lambda x:(x[1],x[0]))\
          .sortByKey(False)\
          .map(lambda x:(x[1],x[0])).take(5)
      sc.stop()


平均数：统计平均年龄
      开发步骤分析：
        1.取出年龄 map
        2.计算年龄总和 reduce
        3.计算记录总数 count
        4.计算平均数


        if len(sys.argv) !=2
          print("Usage: TopN <input>",file = sys.stderr)
          sys.exit(-1)

        conf = SparkConf()
        sc = SparkContext( conf = conf )
        ageData = sc.textFile(sys.argv[1]).map(lambda x:x.split(" ")[1])
        totalAge = ageData.map(lamda age:int(age)).reduce(lambda a,b:a+b)
        counts = ageData.count()
        avgAge = totalAge/counts
